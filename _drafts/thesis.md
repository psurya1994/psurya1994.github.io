Thesis

June 25th:
- Had second meeting with Prof. Blake, expressed my interest in memory. 
	- He explained related work in his lab, going off on tangents whenever I asked questions. Was a very productive discussion and I have a much clearer direction now.
	- Need to learn: RNNs, LSTMs, neural turning machines, information theory (mutual information), papers shared. (for having a productive discussion next time)
	- Could try: implementing what I learn in code, explaining what I learnt on videos
- Reading to better understand neural turning machines. (strategy: reading it like a GRE essay)
	- https://distill.pub/2016/augmented-rnns/
	- from original paper
	- I still only have a vague understanding. Trick: write down which parts you understood and which parts you haven't; helps clear up your minf. (will try tomorrow)
	- Read the paper again carefully for 1.5 hour before sleeping. It's a beautiful idea, was thinking about it even as I went to sleep. Quite interesting!
